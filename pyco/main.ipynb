{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network from Kell, Yamins, Shook, Norman-Haignere, McDermott, 2018\n",
    "\n",
    "This notebook shows how to create a tensorflow graph for the network with the weights and biases used in <a href=\"https://www.cell.com/neuron/fulltext/S0896-6273(18)30250-2\">Kell et al., 2018</a>. This notebook also gives an example of how to pass a sound into the network.\n",
    "\n",
    "\n",
    "### Note on network input\n",
    "\n",
    "The input to the network is a \"cochleagram\", a time-frequency decomposition of a sound that is similar to a spectrogram. Below we provide examples of how to pass a pre-computed cochleagram into the network, as well as how to compute the cochleagram for an example wav and then pass that cochleagram to the network.\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "Most of the dependencies to run this are relatively standard. However, please note the following:\n",
    "- This notebook was tested and run with version 1.5.0 of `tensorflow`. It was not tested with other versions.\n",
    "- `pycochleagram` is a module to generate cochleagrams to pass sounds into the network, which can be found <a href=\"https://github.com/mcdermottLab/pycochleagram\">here</a>.\n",
    "- `PIL` is the Python Image Library.\n",
    "\n",
    "### Contact\n",
    "If you have any questions, please contact Alex Kell. Email: < first_name >< last_name >@mit.edu.\n",
    "\n",
    "Thanks, and enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import IPython.display as ipd\n",
    "import sys\n",
    "sys.path.append('./network/')\n",
    "from branched_network_class import branched_network\n",
    "import tensorflow as tf\n",
    "import scipy.io.wavfile as wav\n",
    "import matplotlib as plt \n",
    "%pylab inline\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the following to run demo_from_wav()\n",
    "import pyco.cochleagram as cgram\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some helper functions\n",
    "def resample(example, new_size):\n",
    "    im = Image.fromarray(example)\n",
    "    resized_image = im.resize(new_size, resample=Image.ANTIALIAS)\n",
    "    return np.array(resized_image)\n",
    "\n",
    "def plot_cochleagram(cochleagram, title): \n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.matshow(cochleagram.reshape(256,256), origin='lower',cmap=plt.cm.Blues, fignum=False, aspect='auto')\n",
    "    plt.yticks([]); plt.xticks([]); plt.title(title); \n",
    "    \n",
    "def play_wav(wav_f, sr, title):   \n",
    "    print title+':'\n",
    "    ipd.display(ipd.Audio(wav_f, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-51f644bf698e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Speech Example ... \\n clean speech, actual label: Cheese, predicted_label: \"\u001b[0m         \u001b[0;34m+\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdemo_from_wav\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/cantonese/cantonese1/also.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-51f644bf698e>\u001b[0m in \u001b[0;36mdemo_from_wav\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdemo_from_wav\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mnet_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbranched_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mword_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./demo_stim/logits_to_word_key.npy'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# load logits to word key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmusic_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./demo_stim/logits_to_genre_key.npy'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# load logits to word key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "def demo_from_wav(filename):\n",
    "    tf.reset_default_graph()\n",
    "    net_object = branched_network()\n",
    "    word_key = np.load('./demo_stim/logits_to_word_key.npy') # load logits to word key\n",
    "    music_key = np.load('./demo_stim/logits_to_genre_key.npy') # load logits to word key \n",
    "    \n",
    "    \n",
    "    # generate cochleagram, then pass cochleagram through network and get logits for word branch\n",
    "    \n",
    "    c_gram = generate_cochleagram(filename)\n",
    "    logits = net_object.session.run(net_object.word_logits, feed_dict={net_object.x: c_gram})\n",
    "    prediction = word_key[np.argmax(logits, axis=1)]\n",
    "    print \"Speech Example ... \\n clean speech, actual label: Cheese, predicted_label: \" \\\n",
    "        + prediction[0] +'\\n'\n",
    "    \n",
    "demo_from_wav('./data/cantonese/cantonese1/also.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 small\n",
      "78 call\n",
      "139 things\n",
      "219 cheese\n",
      "241 store\n",
      "263 need\n",
      "396 these\n",
      "397 also\n",
      "514 into\n",
      "585 fresh\n"
     ]
    }
   ],
   "source": [
    "word_key = np.load('./demo_stim/logits_to_word_key.npy') # load logits to word key\n",
    "allowed_words = [\n",
    "    'also',\n",
    "    'call',\n",
    "    'cheese',\n",
    "    'fresh',\n",
    "    'into',\n",
    "    'need',\n",
    "    'small',\n",
    "    'store',\n",
    "    'these',\n",
    "    'things',\n",
    "]\n",
    "\n",
    "for i, word in enumerate(word_key):\n",
    "    if word in allowed_words:\n",
    "        print i, word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cochleagram(filename):\n",
    "    # define parameters\n",
    "    wav_f, sr = librosa.core.load(filename, sr=16000) # note the sampling rate is 16000hz.\n",
    "    n = 50\n",
    "    low_lim, hi_lim = 20, 8000\n",
    "    sample_factor, pad_factor, downsample = 4, 2, 200\n",
    "    nonlinearity, fft_mode, ret_mode = 'power', 'auto', 'envs'\n",
    "    strict = True\n",
    "    # create cochleagram\n",
    "    c_gram = cgram.cochleagram(wav_f, sr, n, low_lim, hi_lim, \n",
    "                               sample_factor, pad_factor, downsample,\n",
    "                               nonlinearity, fft_mode, ret_mode, strict)\n",
    "\n",
    "    # rescale to [0,255]\n",
    "    c_gram_rescaled =  255*(1-((np.max(c_gram)-c_gram)/np.ptp(c_gram)))\n",
    "    \n",
    "    # reshape to (256,256)\n",
    "    c_gram_reshape_1 = np.reshape(c_gram_rescaled, (211,400))\n",
    "    c_gram_reshape_2 = resample(c_gram_reshape_1,(256,256))\n",
    "\n",
    "    # prepare to run through network -- i.e., flatten it\n",
    "    c_gram_flatten = np.reshape(c_gram_reshape_2, (1, 256*256)) \n",
    "\n",
    "    return c_gram_flatten"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
