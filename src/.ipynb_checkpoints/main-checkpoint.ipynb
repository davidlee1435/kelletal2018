{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network from Kell, Yamins, Shook, Norman-Haignere, McDermott, 2018\n",
    "\n",
    "This notebook shows how to create a tensorflow graph for the network with the weights and biases used in <a href=\"https://www.cell.com/neuron/fulltext/S0896-6273(18)30250-2\">Kell et al., 2018</a>. This notebook also gives an example of how to pass a sound into the network.\n",
    "\n",
    "\n",
    "### Note on network input\n",
    "\n",
    "The input to the network is a \"cochleagram\", a time-frequency decomposition of a sound that is similar to a spectrogram. Below we provide examples of how to pass a pre-computed cochleagram into the network, as well as how to compute the cochleagram for an example wav and then pass that cochleagram to the network.\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "Most of the dependencies to run this are relatively standard. However, please note the following:\n",
    "- This notebook was tested and run with version 1.5.0 of `tensorflow`. It was not tested with other versions.\n",
    "- `pycochleagram` is a module to generate cochleagrams to pass sounds into the network, which can be found <a href=\"https://github.com/mcdermottLab/pycochleagram\">here</a>.\n",
    "- `PIL` is the Python Image Library.\n",
    "\n",
    "### Contact\n",
    "If you have any questions, please contact Alex Kell. Email: < first_name >< last_name >@mit.edu.\n",
    "\n",
    "Thanks, and enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "from network.branched_network_class import branched_network\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "# %pylab inline\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fheee\n"
     ]
    }
   ],
   "source": [
    "# import the following to run demo_from_wav()\n",
    "import cochleagram as cgram\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some helper functions\n",
    "def resample(example, new_size):\n",
    "    im = Image.fromarray(example)\n",
    "    resized_image = im.resize(new_size, resample=Image.ANTIALIAS)\n",
    "    return np.array(resized_image)\n",
    "\n",
    "def plot_cochleagram(cochleagram, title): \n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.matshow(cochleagram.reshape(256,256), origin='lower',cmap=plt.cm.Blues, fignum=False, aspect='auto')\n",
    "    plt.yticks([]); plt.xticks([]); plt.title(title); \n",
    "    \n",
    "def play_wav(wav_f, sr, title):   \n",
    "    print title+':'\n",
    "    ipd.display(ipd.Audio(wav_f, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 small\n",
      "78 call\n",
      "139 things\n",
      "219 cheese\n",
      "241 store\n",
      "263 need\n",
      "396 these\n",
      "397 also\n",
      "514 into\n",
      "585 fresh\n"
     ]
    }
   ],
   "source": [
    "word_key = np.load('./demo_stim/logits_to_word_key.npy') # load logits to word key\n",
    "allowed_words = [\n",
    "    'also',\n",
    "    'call',\n",
    "    'cheese',\n",
    "    'fresh',\n",
    "    'into',\n",
    "    'need',\n",
    "    'small',\n",
    "    'store',\n",
    "    'these',\n",
    "    'things',\n",
    "]\n",
    "\n",
    "for i, word in enumerate(word_key):\n",
    "    if word in allowed_words:\n",
    "        print i, word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cochleagram(filename):\n",
    "    # define parameters\n",
    "    wav_f, sr = librosa.core.load(filename, sr=16000) # note the sampling rate is 16000hz.\n",
    "    n = 50\n",
    "    low_lim, hi_lim = 20, 8000\n",
    "    sample_factor, pad_factor, downsample = 4, 2, 200\n",
    "    nonlinearity, fft_mode, ret_mode = 'power', 'auto', 'envs'\n",
    "    strict = True\n",
    "    # create cochleagram\n",
    "    c_gram = cgram.cochleagram(wav_f, sr, n, low_lim, hi_lim, \n",
    "                               sample_factor, pad_factor, downsample,\n",
    "                               nonlinearity, fft_mode, ret_mode, strict)\n",
    "\n",
    "    # rescale to [0,255]\n",
    "    c_gram_rescaled =  255*(1-((np.max(c_gram)-c_gram)/np.ptp(c_gram)))\n",
    "    \n",
    "    # reshape to (256,256)\n",
    "    c_gram_reshape_1 = np.reshape(c_gram_rescaled, (211,400))\n",
    "    c_gram_reshape_2 = resample(c_gram_reshape_1,(256,256))\n",
    "\n",
    "    # prepare to run through network -- i.e., flatten it\n",
    "    c_gram_flatten = np.reshape(c_gram_reshape_2, (1, 256*256)) \n",
    "\n",
    "    return c_gram_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cochleagram.py:133: RuntimeWarning: divide by zero encountered in log10\n",
      "  freqs_to_plot = np.log10(freqs)\n",
      "/home/davidlee/dev/kelletal2018/venv/local/lib/python2.7/site-packages/scipy/signal/signaltools.py:2383: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return y[keep]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech Example ... \n",
      " clean speech, actual label: Cheese, predicted_label: __null__\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def demo_from_wav(filename):\n",
    "    tf.reset_default_graph()\n",
    "    net_object = branched_network()\n",
    "    word_key = np.load('./demo_stim/logits_to_word_key.npy') # load logits to word key\n",
    "    music_key = np.load('./demo_stim/logits_to_genre_key.npy') # load logits to word key \n",
    "    \n",
    "    \n",
    "    # generate cochleagram, then pass cochleagram through network and get logits for word branch\n",
    "    \n",
    "    c_gram = generate_cochleagram(filename)\n",
    "    logits = net_object.session.run(net_object.word_logits, feed_dict={net_object.x: c_gram})\n",
    "    prediction = word_key[np.argmax(logits, axis=1)]\n",
    "    print \"Speech Example ... \\n clean speech, actual label: Cheese, predicted_label: \" \\\n",
    "        + prediction[0] +'\\n'\n",
    "    \n",
    "demo_from_wav('../data/cantonese/cantonese1/also.wav')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
